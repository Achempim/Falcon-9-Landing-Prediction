{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Environment & Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mSyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 3-4: truncated \\UXXXXXXXX escape. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the CSV file\n",
    "file = 'Processed_SpaceX_Launch_Dataset.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file):\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(file, encoding='utf-8')\n",
    "    # Display the first few rows of the DataFrame\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(f\"âŒ File not found: {file}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns explicitly mentioned\n",
    "columns_to_drop = ['fairings']\n",
    "\n",
    "# Identify columns with more than 50% missing data\n",
    "missing_threshold = 0.5  # 50% threshold\n",
    "cols_to_drop = df.columns[df.isnull().mean() > missing_threshold].tolist()\n",
    "\n",
    "# Combine both drop lists and remove from DataFrame\n",
    "all_columns_to_drop = list(set(columns_to_drop + cols_to_drop))\n",
    "\n",
    "# Ensure columns to drop exist in the DataFrame\n",
    "all_columns_to_drop = [col for col in all_columns_to_drop if col in df.columns]\n",
    "\n",
    "df_cleaned = df.drop(columns=all_columns_to_drop, axis=1)\n",
    "\n",
    "# Handling missing values (replace NaNs with median values)\n",
    "numeric_cols = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "df_cleaned[numeric_cols] = df_cleaned[numeric_cols].fillna(df_cleaned[numeric_cols].median())\n",
    "\n",
    "# Convert categorical variables into numerical values using one-hot encoding\n",
    "categorical_cols = ['Launch Site', 'Weather Condition']\n",
    "existing_categorical_cols = [col for col in categorical_cols if col in df_cleaned.columns]\n",
    "if existing_categorical_cols:\n",
    "\tdf_cleaned = pd.get_dummies(df_cleaned, columns=existing_categorical_cols, drop_first=True)\n",
    "\n",
    "# Display cleaned data\n",
    "display(df_cleaned.head())\n",
    "\n",
    "# Print dropped columns for verification\n",
    "print(f\"\\nâœ… Dropped Columns: {all_columns_to_drop}\")\n",
    "print(f\"âœ… Remaining Columns: {df_cleaned.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Landing Success\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=df['success'])\n",
    "plt.title(\"Success Rate of Falcon 9 Landings\")\n",
    "plt.show()\n",
    "\n",
    "# Ensure 'static_fire_date_unix' column exists\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "numeric_df = df_cleaned.select_dtypes(include=[np.number])\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of Payload Mass\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df_cleaned['static_fire_date_unix'], bins=20, kde=True)\n",
    "plt.title(\"Distribution of Payload Mass - Static Fire Date\")\n",
    "plt.xlabel(\"Payload Mass (kg)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of Payload Mass\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df_cleaned['flight_number'], bins=20, kde=True)\n",
    "plt.title(\"Distribution of Payload Mass - Flight Number\")\n",
    "plt.xlabel(\"Payload Mass (kg)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter Plot: static_fire_date_unix vs Landing Success\n",
    "plt.figure(figsize=(8, 5))\n",
    "# Assuming 'success' column represents the landing outcome\n",
    "sns.scatterplot(x='static_fire_date_unix', y='success', data=df_cleaned)\n",
    "plt.title(\"Static_fire_date_unix vs Landing Success\")\n",
    "plt.xlabel(\"Static_fire_date_unix\")\n",
    "plt.ylabel(\"Landing Success (1=Success, 0=Failure)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection & Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "# Assuming 'success' column represents the landing outcome\n",
    "X = df_cleaned.drop(columns=['success'])\n",
    "\n",
    "# Drop non-numeric columns\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "y = df_cleaned['success']\n",
    "\n",
    "# Split dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Check the columns in df_cleaned\n",
    "print(list(df_cleaned.columns)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Multiple Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear')\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning (Grid Search CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Grid Search for Decision Tree Classifier\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model from Grid Search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest Decision Tree Model:\", best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the best model\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Display Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_best), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mSyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 3-4: truncated \\UXXXXXXXX escape. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_test, y_pred_best)\n",
    "print(\"\\nðŸ“Œ Classification Report:\\n\", report)\n",
    "\n",
    "# Accuracy Score\n",
    "accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"\\nâœ… Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Precision, Recall, F1-Score\n",
    "precision = precision_score(y_test, y_pred_best)\n",
    "recall = recall_score(y_test, y_pred_best)\n",
    "f1 = f1_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"ðŸ”¹ Precision: {precision:.4f}\")\n",
    "print(f\"ðŸ”¹ Recall: {recall:.4f}\")\n",
    "print(f\"ðŸ”¹ F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
